# MIPCA (MIPCA: Multiple Imputation with PCA)

M√°s detalles ‚Üí [Documentaci√≥n](https://www.rdocumentation.org/packages/missMDA/versions/1.20/topics/MIPCA)

---

## PCA (An√°lisis de Componentes Principales)  
Es una forma de mirar los datos desde el mejor √°ngulo posible, donde se ve la mayor diferencia entre ellos.  
As√≠ puedes resumir tus datos en menos ‚Äú√°ngulos‚Äù, pero sin perder tanta informaci√≥n.

---

## C√≥mo funciona paso a paso

1. **Restar la media (centrar los datos)**  
   - Imagina que tienes varias reglas en la mesa con diferentes medidas üìè.  
   - Primero, mueves todas las reglas para que empiecen en el mismo lugar (como llevar todas al 0 de la regla).  
   - Eso es **restar la media**.  

2. **Buscar los √°ngulos donde hay m√°s diferencia**  
   - Piensa en muchos puntos dibujados en un papel ‚úèÔ∏è.  
   - PCA dibuja una l√≠nea en el papel que pasa por donde los puntos se estiran m√°s (como una flecha en la direcci√≥n m√°s larga de la nube de puntos).  
   - Esa flecha es el **primer componente principal**.  
   - Luego busca otra flecha en la siguiente direcci√≥n m√°s importante (segunda componente).  

3. **Guardar esos √°ngulos**  
   - Cada flecha que encontramos se llama **componente**.  
   - Cu√°nto ‚Äúexplica‚Äù esa flecha se llama **varianza explicada** (es como decir: *‚Äúesta flecha cuenta el 70% de la historia de mis datos‚Äù*).  

4. **Transformar los datos**  
   - Ahora que tenemos las flechas (componentes), podemos proyectar nuestros puntos en esas flechas.  
   - Es como si en vez de guardar toda la foto en 3D, la dibuj√°ramos solo en la flecha m√°s importante en 2D.  
   - üéØ As√≠ resumimos los datos en menos dimensiones.  

---

## C√≥mo lo ponemos en Python

Escribimos una clase llamada **MIPCA**, que tiene 3 s√∫per poderes:  
1. `fit` (ajustar) ‚Üí aprende cu√°les son las mejores flechas (componentes) de los datos.  
2. `transform` (transformar) ‚Üí usa esas flechas para dibujar los datos en un espacio m√°s peque√±o.  
3. `explained_variance` (varianza explicada) ‚Üí te dice qu√© tanto explica cada flecha de la historia.  

```python
import numpy as np

# Supongamos que tenemos datos de 100 ni√±os con 5 notas diferentes
X = np.random.rand(100, 5)

# Creamos nuestro PCA casero
pca = MIPCA(n_components=2)

# Le pedimos que aprenda las mejores "flechas"
pca.fit(X)

# Transformamos los datos a 2 dimensiones
Z = pca.transform(X)

print("Varianza explicada:", pca.explained_variance_ratio_)
```

**üëâ Al final nos dice:**

	-	‚ÄúLa primera flecha explica el 60% de la historia‚Äù
	-	‚ÄúLa segunda flecha explica el 30%‚Äù
	-	Total = 90% (muy bien, ya sabemos casi toda la historia con solo 2 flechas üéâ).

## Analog√≠a final

Imagina que tienes un rompecabezas gigante con 1000 piezas üß©.
PCA te ayuda a resumirlo en pocas piezas grandes, que aunque no sean todas las piezas originales,
s√≠ muestran casi toda la imagen.
